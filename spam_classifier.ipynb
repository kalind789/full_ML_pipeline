{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_path = \"~/downloads/sms+spam+collection/SMSSpamCollection\"\n",
    "df = pd.read_csv(data_path, sep=\"\\t\", header=None, names=[\"label\", \"text\"])\n",
    "print(df.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is imbalanced due to the proportion of positive to negative labels, which will determine the choice of metrics in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Holdout Set\n",
    "\n",
    "Most tutorials use test/train splits for easier data processing, but in production a holdout set of unseen cases helps evaluate your model more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting data into a training validation set and a holdout set\n",
    "df_train_val, df_holdout = train_test_split(df, \n",
    "                                            test_size=0.1,  # splits 10% of the data into the holdout set\n",
    "                                            stratify=df['label'],   # stratify ensures that the correct class distribution is preserved\n",
    "                                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_val.to_csv('./data/raw/spam_train_val.csv', index=False)\n",
    "df_holdout.to_csv('./data/raw/spam_holdout.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Text\n",
    "\n",
    "Text data is noisy in terms of \"win\" vs \"winning\" and other nuances that are not important for the meaning of the text, especially in the context of spam detection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ntlk'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mntlk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords   \u001b[38;5;66;03m# Words like \"the\", \"a\", \"but\", \"and\" w/ no meaning \u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstem\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer \u001b[38;5;66;03m# Stemming = removing the \"ing\" so the meaning is still preserved\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ntlk'"
     ]
    }
   ],
   "source": [
    "from ntlk.corpus import stopwords   # Words like \"the\", \"a\", \"but\", \"and\" w/ no meaning \n",
    "from nltk.stem import PorterStemmer # Stemming = removing the \"ing\" so the meaning is still preserved\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string, nltk\n",
    "\n",
    "nltk.download('punkt_tab')  # Punctuation\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))    # Remove punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "    return ''.join(tokens)\n",
    "\n",
    "df_train_val['clean_text'] = df_train_val['text'].apply(preprocess_text)\n",
    "df_train_val['label_num'] = df_train_val['label'].map({'ham': 0, 'spam': 1})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
